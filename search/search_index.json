{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Analitiq","text":"<p>Analitiq is a comprehensive framework designed to enhance data management using Large Language Models (LLMs). Our goal is to transcend traditional Text-to-SQL functionalities, enabling Data Engineers, Analysts, and non-technical users to interact with data effortlessly through natural language and advanced AI capabilities.</p> <p>With Analitiq, you can harness the power of LLMs to:</p> <ul> <li>Query data from your Data Warehouse.</li> <li>Search for information across your company wiki, SQL files, and Data Warehouse schema.</li> <li>Create custom agents tailored to your data management needs.</li> <li>Utilize your preferred LLM, whether it's a private model via Ollama or public APIs like OpenAI and Mistral.</li> </ul>"},{"location":"#deployment-options","title":"Deployment Options","text":"<ul> <li>Self-hosted: Self-hosted Analitiq option will let you run it in your own environment.</li> <li>Cloud: Analitiq cloud is a managed service for Enterprise customers.</li> </ul>"},{"location":"#user-interfaces-for-analitiq-cloud","title":"User Interfaces for Analitiq Cloud","text":""},{"location":"#slack","title":"Slack","text":"<p>Analitiq Cloud can be added to a Slack account to allow anyone in the organisation to interact with data using LLMs. </p>"},{"location":"#web-ui","title":"Web UI","text":"<p>Analitiq Cloud can be accessed via our UI interface. </p>"},{"location":"#supported-databases","title":"Supported Databases","text":"<p>Analitiq currently supports the following LLM models - Postgres - Redshift</p>"},{"location":"#supported-llms","title":"Supported LLMs","text":"<p>Analitiq currently supports the following LLM models - ChatGPT - Mistral - Bedrock (AWS)</p>"},{"location":"#supported-vector-databases","title":"Supported Vector Databases","text":"<p>Analitiq currently integrates with the following vectorDBs - Weaviate - ChromaDB</p>"},{"location":"#usage-examples","title":"Usage Examples","text":""},{"location":"#keywords","title":"Keywords","text":"<ul> <li><code>FAIL</code> - this keyword will instruct Analitiq that response was not what was expected. This will cause Analitiq to record the chat in a <code>failed.log</code> file. </li> </ul>"},{"location":"#what-analitiq-needs-to-work","title":"What Analitiq needs to work","text":"<p>Since Analitiq is a framework to help data people manage data using LLMs, it requires at the least:</p> <ol> <li>Access to LLM(s) - required</li> <li>Access to Database - required</li> <li>Access to Vector Database with documentation - optional</li> </ol>"},{"location":"agents/doc_search/","title":"Document Search Agent","text":"<p>The Document Search agent, as part of the <code>Analitiq</code> framework, provides users with the ability to upload all their documents into a search database and utilize it as a context for data analysis. This agent highlights the power of advanced search algorithms, natural language processing (NLP), and indexing to deliver efficient and context-specific search results.</p>"},{"location":"agents/doc_search/#purpose","title":"Purpose","text":"<p>The main purpose of the Document Search agent is to simplify and accelerate context-based data discovery and analysis for the users. With the power of full-text search, users can readily search their textual documents, find meaningful insights, and improve the effectiveness of their decision-making processes.</p> <p>This agent eliminates the need for manual tracking and search through extensive amounts of documents, making it an ideal tool for knowledge workers, researchers, and anyone who needs to effectively navigate and analyze their document datasets.</p>"},{"location":"agents/doc_search/#specifications","title":"Specifications","text":"<p>Through the Document Search agent, users can load their documents into the search database. Post indexing of these documents, the agent allows searching within the uploaded documents seamlessly. The document search agent yields relevant search results based on context, enabling users to find appropriate content for data analytics and decision-making.</p> <p>Users can interact with this agent using natural language inputs, making it accessible to users without a deep understanding of search algorithms. This process simplifies data discovery and equips the users with an intuitive interface to interact with their documents.</p>"},{"location":"agents/doc_search/#how-it-works","title":"How It Works","text":"<p>The Document Search agent operates through several key stages:</p> <ol> <li>User Query Processing: The user issues a search query in natural language, which is then processed and understood by the agent.</li> <li>Document Repository Search: The agent sends the refined query to the document repository where all documents are stored and indexed.</li> <li>Result Aggregation: The agent receives the search results, which are then strategically aggregated. The system uses advanced techniques such as Natural Language Processing (NLP) and machine learning to perform this action.</li> <li>Result Ranking: After aggregating the results, the agent ranks them based on their relevance to the user's search query. This ranking system ensures that the most relevant documents appear at the top of the results.</li> <li>Interpretation by the LLM (Language Learning Model): The LLM interprets the aggregated and ranked results. Its job is to understand the context of the documents in the search results.</li> <li>Summarised Information Presentation: The LLM converts the interpreted results into a summarized form, which is then presented to the user. This summarization makes it easier for the user to get the crux of the search results quickly.</li> </ol>"},{"location":"agents/doc_search/#inputs-and-outputs","title":"Inputs and Outputs","text":""},{"location":"agents/doc_search/#inputs","title":"Inputs","text":"<ul> <li>User Prompt (str): A natural language description of the data retrieval or analysis task.</li> </ul>"},{"location":"agents/doc_search/#outputs","title":"Outputs","text":""},{"location":"agents/doc_search/#-response-object-contains-the-text-summary-of-the-documents-and-the-interpretation-by-the-llm","title":"- Response Object: Contains the text summary of the documents and the interpretation by the LLM.","text":"<ol> <li>response.content = pandas dataframe of the result</li> <li>response.metadata = metadata, such as SQL that has been executed. <pre><code>metadata={\n    \"chart_type\": \"Pie\"\n}\n</code></pre></li> </ol>"},{"location":"agents/doc_search/#example-direct-usage","title":"Example direct usage.","text":"<p>We do not recommend using this module outside of the Analitiq framework. However, if you would like to load it directly, here is how you can do it.</p> <p><pre><code>from analitiq.vector_databases.weaviate import WeaviateHandler\n\nparams = {\n    \"project_name\": \"my_project\",\n    \"host\": \"https://XXXXXXX.weaviate.network\",\n    \"api_key\": \"XXXXXX\"\n}\n\nvdb = WeaviateHandler(params)\n</code></pre> We need 1 thing for Document Search agent to work with:</p> <ol> <li>VectorDB (for searching documentation for better SQL results)</li> </ol> <p>Trigger search with user query: <pre><code>search_results = vdb.kw_search(\"climate change\", limit=5)\n</code></pre></p> <p>Returned Data Format:</p> <pre><code>{\n    \"document_id_1\": {\n        \"content\": \"Document content related to climate change...\",\n        \"document_name\": \"Document 1\",\n        \"source\": \"Source A\"\n    },\n    \"document_id_2\": {\n        \"content\": \"Another document content about climate change...\",\n        \"document_name\": \"Document 2\",\n        \"source\": \"Source B\"\n    }\n}\n</code></pre>"},{"location":"agents/sql/","title":"SQL agent","text":"<p>The SQL agent, part of the <code>analitiq</code> framework, empowers users to interact with databases through natural language prompts. By leveraging advanced natural language processing (NLP) techniques, this agent interprets user inputs to determine relevant database tables, generate SQL queries accordingly, and execute these queries. The results are returned as pandas DataFrames, making data analysis and visualization tasks more accessible to users without deep SQL expertise.</p>"},{"location":"agents/sql/#purpose","title":"Purpose","text":"<p>The SQL agent is designed to bridge the gap between natural language queries and SQL, enabling users to extract and analyze data from databases by simply describing their data retrieval needs in plain English. This eliminates the need for intricate SQL knowledge and speeds up the data retrieval process, making it ideal for quick insights and iterative data exploration.</p>"},{"location":"agents/sql/#how-it-works","title":"How It Works","text":"<p>The agent operates through several steps:</p> <ol> <li>Table Identification: From the user's natural language prompt, the agent identifies relevant tables within the database.</li> <li>Query Generation: It then translates the prompt into a structured SQL query using the identified tables.</li> <li>Query Execution: The SQL query is executed against the database, and the results are fetched.</li> <li>Result Transformation: Finally, the results are converted into a pandas DataFrame for easy manipulation, analysis, or visualization.</li> </ol>"},{"location":"agents/sql/#inputs-and-outputs","title":"Inputs and Outputs","text":""},{"location":"agents/sql/#inputs","title":"Inputs","text":"<ul> <li>User Prompt (str): A natural language description of the data retrieval or analysis task.</li> </ul>"},{"location":"agents/sql/#outputs","title":"Outputs","text":"<ul> <li>Response Object: Contains the query result set as a pandas DataFrame and metadata including executed SQL and relevant tables.</li> <li>response.content = pandas dataframe of the result</li> <li>response.metadata = metadata, such as SQL that has been executed. <pre><code>metadata={\n    \"chart_type\": \"Pie\"\n}\n</code></pre></li> </ul>"},{"location":"agents/sql/#example-usage","title":"Example Usage","text":"<p>You may need to add directory for Analitiq to your Python path. Here is an example: <pre><code>import os\nimport sys\n# Get the home directory\nhome_directory = os.environ['HOME']\n# Dynamically construct the path\ndynamic_path = f'{home_directory}/Documents/Projects/analitiq/libs/'\nsys.path.insert(0, dynamic_path)\n</code></pre> We need 3 things for SQL agent to work with: 1. Database 2. Large Language Model 3. VectorDB (for searching documentation for better SQL results)</p> <pre><code>from sql import Sql\nfrom analitiq.base.Database import DatabaseWrapper\nfrom analitiq.base.llm.BaseLlm import BaseLlm\nfrom analitiq.base.vectordb.weaviate.weaviate_vs import WeaviateHandler\n\nuser_prompt = \"Please give me revenues by month.\"\n\ndb_params = {'name': 'prod_dw'\n    , 'type': 'redshift'\n    , 'host': 'xxxx'\n    , 'username': 'smy_user'\n    , 'password': '1234455'\n    , 'port': 5439\n    , 'db_name': 'my_db'\n    , 'db_schemas': ['schema1', 'schema2']\n    , 'threads': 4\n    , 'keepalives_idle': 240\n    , 'connect_timeout': 10}\ndb = DatabaseWrapper(db_params)\n\nllm_params = {'type': 'bedrock'\n    , 'name': 'aws_llm'\n    , 'api_key': None\n    , 'temperature': 0.0\n    , 'llm_model_name': 'anthropic.claude-v2'\n    , 'credentials_profile_name': 'my_profile'\n    , 'provider': 'anthropic'\n    , 'aws_access_key_id': 'xxxxxx'\n    , 'aws_secret_access_key': 'xxxxxxx'\n    , 'region_name': 'eu-central-1'}\nllm = BaseLlm(llm_params)\n\nvdb = WeaviateHandler('https://12345.weaviate.network', 'xxxxxxx', 'my_project')\n\n# Example of using the SQLGenerator class\nsql_gen = Sql(\"Please give me revenues by month.\", db, llm, vector_db=vdb)\nresult = sql_gen.run()\nprint(result)\n</code></pre>"},{"location":"cookbooks/load_documents/","title":"Loading Documents into Analitiq Cloud Repository","text":"<p>Use the Analitiq application to load documents into your cloud repository to give Analitiq more context to work with and answer questions effectively. Follow the steps below to load documents and utilize the Analitiq capabilities.</p>"},{"location":"cookbooks/load_documents/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have the necessary parameters: - collection_name: Your collection name (obtained from Analitiq support team) - host: The host address of your Weaviate instance (obtained from Analitiq support team) - api_key: Your API key (obtained from Analitiq support team)</p>"},{"location":"cookbooks/load_documents/#code-snippets","title":"Code Snippets","text":""},{"location":"cookbooks/load_documents/#1-initialize-the-weaviate-handler","title":"1. Initialize the Weaviate Handler","text":"<pre><code>from libs.analitiq.vector_databases.weaviate import WeaviateHandler\n\nparams = {\n    \"collection_name\": \"xxx\",\n    \"host\": \"xxxxx\",\n    \"api_key\": \"xxxx\"\n}\n\nvdb = WeaviateHandler(params)\n</code></pre>"},{"location":"cookbooks/load_documents/#2-load-a-directory","title":"2. Load a Directory","text":"<p>To load all files from a directory, use the following code:</p> <pre><code># Load a directory\nFILE_PATH = '/xxx/xxx/xxx/xxx/models'\nvdb.load(FILE_PATH, 'sql')\n</code></pre>"},{"location":"cookbooks/load_documents/#3-load-a-single-file","title":"3. Load a Single File","text":"<p>To load a single file, use the following code:</p> <pre><code># Load a single file\nFILE_PATH = '/xxx/xxx/xxx/xxx/models'\nvdb.load(FILE_PATH)\n</code></pre>"},{"location":"cookbooks/load_documents/#4-search-for-results","title":"4. Search for Results","text":"<p>To search for specific keywords in your loaded documents, use the following code:</p> <pre><code># Search for results\nresult = vdb.kw_search(\"bike\")\nprint(result)\n</code></pre>"},{"location":"cookbooks/load_documents/#5-delete-a-collection","title":"5. Delete a Collection","text":"<p>To delete a collection, use the following code:</p> <pre><code># Delete a collection\nvdb.delete_collection(params['collection_name'])\n</code></pre> <p>If you encounter any issues or need assistance, please visit our Support Page or contact us at support@analitiq-app.com.</p> <p>We are here to help you make the most of your data with Analitiq!</p> <p>Thank you for choosing Analitiq!</p> <p>The Analitiq Team</p>"},{"location":"framework/vector_databases/weaviate/","title":"Analitiq::Weaviate Documentation","text":"<p>This documentation covers the <code>WeaviateHandler</code> module, part of the larger Analitiq framework. This module facilitates interactions with a Weaviate vector database, including loading document chunks and performing searches.</p>"},{"location":"framework/vector_databases/weaviate/#overview","title":"Overview","text":"<p>The <code>WeaviateHandler</code> module provides a set of tools for managing and querying a Weaviate vector database. It allows users to load documents, split them into chunks, and perform keyword searches. This module is designed to integrate seamlessly with other parts of the Analitiq framework.</p> <p>Here's a simple example demonstrating the basic functionality of the <code>WeaviateHandler</code> module:</p> <pre><code>from analitiq.vector_databases.weaviate import WeaviateHandler\n\nparams = {\n    \"project_name\": \"my_project\",\n    \"host\": \"https://XXXXXXX.weaviate.network\",\n    \"api_key\": \"XXXXXX\"\n}\n\nvdb = WeaviateHandler(params)\nFILE_PATH = './project/My_Project/sql'\nvdb.load(FILE_PATH, 'sql')\n</code></pre>"},{"location":"framework/vector_databases/weaviate/#modules","title":"Modules","text":""},{"location":"framework/vector_databases/weaviate/#decorators","title":"Decorators","text":""},{"location":"framework/vector_databases/weaviate/#search_only","title":"<code>search_only</code>","text":"<p>This decorator wraps a function to ensure it only performs search operations.</p> <pre><code>def search_only(func):\n    def wrapper(*args, **kwargs):\n        return func(*args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"framework/vector_databases/weaviate/#search_grouped","title":"<code>search_grouped</code>","text":"<p>This decorator wraps a function to group the search response by document and source.</p> <pre><code>def search_grouped(func):\n    def wrapper(*args, **kwargs):\n        response = func(*args, **kwargs)\n        self = args[0]\n        return self._group_by_document_and_source(response)\n    return wrapper\n</code></pre>"},{"location":"framework/vector_databases/weaviate/#chunk-class","title":"Chunk Class","text":"<p>The <code>Chunk</code> class represents a chunk of text in a document.</p> <pre><code>class Chunk(BaseModel):\n    project_name: str = None\n    document_name: str = None\n    document_type: Optional[str] = None\n    content: str = None\n    source: str\n    document_num_char: int\n    chunk_num_char: int\n</code></pre>"},{"location":"framework/vector_databases/weaviate/#weaviatehandler-class","title":"WeaviateHandler Class","text":"<p>The <code>WeaviateHandler</code> class handles interactions with a Weaviate cluster and manages a collection within the cluster.</p>"},{"location":"framework/vector_databases/weaviate/#initialization","title":"Initialization","text":"<pre><code>def __init__(self, params):\n    super().__init__(params)\n    if not self.try_connect():\n        self.connected = False\n        self.collection = None\n    else:\n        multi_collection = self.client.collections.get(self.collection_name)\n        self.collection = multi_collection.with_tenant(self.collection_name)\n    self.chunk_processor = DocumentChunkLoader(self.collection_name)\n</code></pre>"},{"location":"framework/vector_databases/weaviate/#methods","title":"Methods","text":"<ul> <li>connect: Connect to the Weaviate database.</li> <li>create_collection: Create a new collection in Weaviate.</li> <li>close: Close the Weaviate client connection.</li> <li>_chunk_load_file_or_directory: Load files from a directory or a single file, split them into chunks, and insert them into Weaviate.</li> <li>load: Load a file or directory into Weaviate.</li> <li>_group_by_document_and_source: Group a list of dictionaries by their 'document_name' and 'source'.</li> <li>kw_search: Perform a keyword search in the Weaviate database.</li> <li>delete_many_like: Delete multiple documents from the collection where the given property value is similar.</li> <li>get_many_like: Retrieve objects from the collection that have a property whose value matches the given pattern.</li> <li>delete_collection: Delete a collection and all its data.</li> </ul>"},{"location":"framework/vector_databases/weaviate/#usage-examples","title":"Usage Examples","text":""},{"location":"framework/vector_databases/weaviate/#loading-files","title":"Loading Files","text":""},{"location":"framework/vector_databases/weaviate/#loading-all-sql-files-from-a-directory","title":"Loading all SQL files from a directory","text":"<pre><code>from analitiq.vector_databases.weaviate import WeaviateHandler\n\nparams = {\n    \"project_name\": \"my_project\",\n    \"host\": \"https://XXXXXXX.weaviate.network\",\n    \"api_key\": \"XXXXXX\"\n}\n\nvdb = WeaviateHandler(params)\nFILE_PATH = './project/My_Project/sql'\nvdb.load(FILE_PATH, 'sql')\n</code></pre>"},{"location":"framework/vector_databases/weaviate/#loading-a-single-file","title":"Loading a single file","text":"<pre><code>from analitiq.vector_databases.weaviate import WeaviateHandler\n\nparams = {\n    \"project_name\": \"my_project\",\n    \"host\": \"https://XXXXXXX.weaviate.network\",\n    \"api_key\": \"XXXXXX\"\n}\n\nvdb = WeaviateHandler(params)\nFILE_PATH = './project/My_Project/my_file.sql'\nvdb.load(FILE_PATH)\n</code></pre>"},{"location":"framework/vector_databases/weaviate/#searching-for-data","title":"Searching for Data","text":""},{"location":"framework/vector_databases/weaviate/#keyword-search","title":"Keyword Search","text":"<p>The <code>kw_search</code> method performs a keyword search in the database.</p> <pre><code>search_results = vdb.kw_search(\"climate change\", limit=5)\n</code></pre> <p>Returned Data Format:</p> <pre><code>{\n    \"document_id_1\": {\n        \"content\": \"Document content related to climate change...\",\n        \"document_name\": \"Document 1\",\n        \"source\": \"Source A\"\n    },\n    \"document_id_2\": {\n        \"content\": \"Another document content about climate change...\",\n        \"document_name\": \"Document 2\",\n        \"source\": \"Source B\"\n    }\n}\n</code></pre>"},{"location":"framework/vector_databases/weaviate/#grouped-search-results","title":"Grouped Search Results","text":"<p>The <code>kw_search_grouped</code> method groups the search results based on their document_name and source.</p> <pre><code>grouped_results = vdb.kw_search_grouped(\"sustainable energy\", limit=5)\n</code></pre> <p>Returned Data Format:</p> <pre><code>{\n    \"('Document 1', 'Source A')\": [\n        \"Document content related to sustainable energy...\",\n        \"Another piece of content from the same document and source...\"\n    ],\n    \"('Document 2', 'Source B')\": [\n        \"Document content on sustainable energy from a different source...\"\n    ]\n}\n</code></pre> <p>These functionalities provide an efficient and flexible way to search and analyze documents in the Weaviate database. Whether you need a straightforward list of search results or a grouped view based on specific attributes, the <code>WeaviateHandler</code> class caters to both requirements seamlessly.</p>"},{"location":"getting_started/cloud/analitiq_add_data_warehouse/","title":"Adding Access to Your Data Warehouse in Analitiq","text":"<p>Follow these steps to add access to your Data Warehouse in Analitiq. This will enable Analitiq to access and analyze your data efficiently.</p>"},{"location":"getting_started/cloud/analitiq_add_data_warehouse/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li>Navigate to Data Connections</li> </ol> <p>Once you are logged in to the Analitiq Web UI, navigate to the Database Connections section.</p> <p></p> <ol> <li>Add a New Connection</li> </ol> <p>Click on the Add Database button to start adding a new connection:</p> <p></p> <ol> <li>Fill in the Connection Details</li> </ol> <p>Provide the necessary details for your Data Warehouse connection:</p> <ul> <li>Connection name: A name for your connection.</li> <li>Database: Select your database type from the dropdown menu.</li> <li>Host: The hostname or IP address of your database server.</li> <li>Port: The port number on which your database server is running.</li> <li>Database name: The name of your database.</li> <li>Username: Your database username.</li> <li>Password: Your database password.</li> <li> <p>Schema: The schema you want to connect to (default is 'public').</p> </li> <li> <p>Test the Connection</p> </li> </ul> <p>After filling in all the required details, click on the Test button to ensure that the connection to your Data Warehouse is working correctly.</p> <ol> <li>Save the Connection</li> </ol> <p>If the connection test is successful, click Save to store your credentials and complete the setup.</p>"},{"location":"getting_started/cloud/analitiq_add_data_warehouse/#important-notes","title":"Important Notes","text":"<ul> <li> <p>Ensure that you connect only the schema containing the final fact and dimension tables ready for data analysis. This ensures that Analitiq focuses on using ready-made tables and does not inadvertently scan tables not intended for consumption.</p> </li> <li> <p>If you encounter any issues during the connection process, please check your connection details or consult your database administrator.</p> </li> </ul> <p>For further assistance, visit our Support Page or contact us at support@analitiq-app.com.</p> <p>We are here to help you make the most of your data with Analitiq!</p> <p>Thank you for choosing Analitiq!</p> <p>The Analitiq Team</p>"},{"location":"getting_started/cloud/analitiq_add_documents/","title":"Adding Documents to Analitiq","text":"<p>In Analitiq, you can add documents to enhance the capabilities of Analitiq by providing it with relevant context and data. This guide will show you how to add documents using the Analitiq Web UI and the Analitiq Python application.</p>"},{"location":"getting_started/cloud/analitiq_add_documents/#adding-documents-via-analitiq-web-ui","title":"Adding Documents via Analitiq Web UI","text":"<p>To upload documents one by one using the Analitiq Web UI, follow these steps:</p> <ol> <li>Navigate to the Documents Section</li> </ol> <p>Once logged in to the Analitiq dashboard, navigate to the Documents section from the menu.</p> <p></p> <ol> <li>Upload a Document</li> </ol> <p>Click on the Upload Document button and you will see a form similar to the one shown below:</p> <p></p> <ol> <li> <p>Fill in the Document Details</p> </li> <li> <p>Content: Paste the content of your document.</p> </li> <li> <p>Document name: Provide a name for your document.</p> </li> <li> <p>Save the Document</p> </li> </ol> <p>Click Save to upload and store your document in Analitiq.</p>"},{"location":"getting_started/cloud/analitiq_add_documents/#adding-multiple-documents-via-analitiq-python-application","title":"Adding Multiple Documents via Analitiq Python Application","text":"<p>To upload multiple documents at once, you can use the Analitiq Python application. The following code snippet, available in the Analitiq Cookbooks, demonstrates how to scan a whole directory of objects and upload them to the VectorDatabase:</p> <pre><code>from libs.analitiq.vector_databases.weaviate import WeaviateHandler\n\nparams = {\n    \"collection_name\": \"xxx\",\n    \"host\": \"xxxxx\",\n    \"api_key\": \"xxxx\"\n}\n\nvdb = WeaviateHandler(params)\n\n# To load all SQL files from a directory\nDIR_PATH = '/xxx/xxx/xxx/xxx/models'\nvdb.load(DIR_PATH, 'sql')\n</code></pre>"},{"location":"getting_started/cloud/analitiq_add_documents/#steps-to-use-the-python-application","title":"Steps to Use the Python Application:","text":"<ol> <li>Install the necessary libraries: Ensure you have the required libraries installed in your Python environment.</li> <li>Configure Parameters: Replace the placeholder values in <code>params</code> with your actual collection name, host, and API key. You can obtain these from Analitiq support.</li> <li>Set Directory Path: Update <code>DIR_PATH</code> with the path to the directory containing your <code>.sql</code> files.</li> <li>Run the Script: Execute the script to upload all <code>.sql</code> files in the specified directory and its subdirectories to the VectorDatabase.</li> </ol> <p>If you encounter any issues or need assistance, please visit our Support Page or contact us at support@analitiq-app.com.</p> <p>We are here to help you make the most of your data with Analitiq!</p> <p>Thank you for choosing Analitiq!</p> <p>The Analitiq Team</p>"},{"location":"getting_started/cloud/analitiq_add_to_slack/","title":"Adding Analitiq to Slack","text":"<p>Integrate Analitiq with your Slack workspace to query your documents or data directly using Slack commands. Follow the steps below to install and use the Analitiq Slack app.</p>"},{"location":"getting_started/cloud/analitiq_add_to_slack/#step-1-install-analitiq-slack-app","title":"Step 1 Install Analitiq Slack App","text":"<ol> <li> <p>Click on the following link to start the installation process: Install Analitiq Slack App.</p> </li> <li> <p>You will be redirected to the Slack authorization page. Select the Slack workspace where you want to install Analitiq.</p> </li> <li> <p>Review the permissions requested by Analitiq and click Allow to complete the installation.</p> </li> </ol>"},{"location":"getting_started/cloud/analitiq_add_to_slack/#step-2-using-analitiq-in-slack","title":"Step 2 Using Analitiq in Slack","text":"<p>Once installed, Analitiq will be available as a standalone app within your Slack workspace. You can interact with Analitiq using Slack commands to query your documents or data.</p> <p></p>"},{"location":"getting_started/cloud/analitiq_slack_commands/","title":"Using Slack Slash Commands to Query Analitiq Agents","text":"<p>This guide will help you understand how to use Slack slash commands to query your documents or data using Analitiq. Analitiq Slack app provides several useful slash commands to help you query and interact with your data. Below are the available commands and their usage:</p>"},{"location":"getting_started/cloud/analitiq_slack_commands/#docs","title":"<code>/docs</code>","text":"<p>Search documentation within your connected documents.</p> <ul> <li>Command: <code>/docs</code></li> <li>Description: Search documentats using Analitiq AI agent</li> <li>Usage: <code>/docs [text to search for]</code></li> </ul> <p></p>"},{"location":"getting_started/cloud/analitiq_slack_commands/#sql","title":"<code>/sql</code>","text":"<p>Query and analyze data in your database using natural language.</p> <ul> <li>Command: <code>/sql</code></li> <li>Description: Query and analyze data in database</li> <li>Usage: <code>/sql [Write your query in plain language]</code></li> </ul> <p></p>"},{"location":"getting_started/cloud/analitiq_slack_commands/#example-usage","title":"Example Usage","text":"<p>Here are some examples of how you can use these commands in Slack:</p>"},{"location":"getting_started/cloud/analitiq_slack_commands/#example-1-querying-revenues-by-month","title":"Example 1: Querying Revenues by Month","text":"<p>User enters the following command in Slack:</p> <pre><code>/sql Please give me revenues by month\n</code></pre> <p>Analitiq responds with the query results and the corresponding SQL query:</p> <p></p>"},{"location":"getting_started/cloud/analitiq_slack_commands/#example-2-searching-documentation","title":"Example 2: Searching Documentation","text":"<p>User enters the following command in Slack:</p> <pre><code>/docs How do we calculate event attendance?\n</code></pre> <p></p> <p></p> <p>Analitiq responds with relevant documentation excerpts matching the search text.</p> <p></p> <p>If you encounter any issues or need assistance, please visit our Support Page or contact us at support@analitiq-app.com.</p> <p>We are here to help you make the most of your data with Analitiq!</p> <p>Thank you for choosing Analitiq!</p> <p>The Analitiq Team</p>"},{"location":"getting_started/cloud/registration/","title":"Getting Started with the Cloud [beta]","text":""},{"location":"getting_started/cloud/registration/#register-for-cloud-version","title":"Register for Cloud Version","text":"<p>To begin using Analitiq, you need to register for our beta cloud version. Please follow these steps:</p> <ol> <li>Visit analitiq-app.com.</li> <li>Click on the Sign Up button.</li> <li>Enter your desired username and password.</li> <li>Click Register to create your account.</li> </ol> <p></p>"},{"location":"getting_started/cloud/registration/#connect-data-warehouse","title":"Connect Data Warehouse","text":"<p>Once registered, you will need to connect your Data Warehouse to Analitiq. This would let you query your data and get inisghts.</p> <ol> <li>Navigate to the Data Connections section in the Analitiq dashboard.</li> <li> <p>Add a Connection by providing the necessary details of your Data Warehouse.</p> </li> <li> <p>It is advisable to connect only the schema containing the final fact and dimension tables ready for data analysis. This ensures that Analitiq focuses on using ready-made tables and does not inadvertently scan tables not intended for consumption.</p> </li> </ol>"},{"location":"getting_started/cloud/registration/#upload-documentation","title":"Upload Documentation","text":"<p>Analitiq Cloud includes a vector database where you can upload documents that Large Language Models (LLMs) can reference for better understanding. By default, Analitiq is connected to a secure Weaviate cluster. This will give Analitiq better understanding of your data. This documentation should ideally describe your tables and columns. An example of such documentation is a <code>dbt schema.yml</code> file.</p> <ol> <li>Go to the Documentation Upload section in the Analitiq dashboard.</li> <li>Click Upload Document and select your documentation file.</li> </ol>"},{"location":"getting_started/cloud/registration/#start-querying-and-interacting-with-analitiq","title":"Start Querying and Interacting with Analitiq","text":"<p>Once your Data Warehouse is connected and documentation is uploaded, you can start querying and interacting with Analitiq using the web interface. Our pre-defined LLM operates in read-only mode and ensures that none of your data is used for training.</p> <p></p>"},{"location":"getting_started/cloud/registration/#key-features","title":"Key Features","text":"<ul> <li>Beta Cloud Version: Register easily and start using Analitiq's powerful data analytics capabilities.</li> <li>Secure Data Connection: Connect only the necessary schemas to ensure focused and efficient data analysis.</li> <li>Vector Database: Upload documents to enhance the LLM's understanding and reference.</li> <li>Pre-defined LLM: Operates in read-only mode ensuring your data remains secure and is never used for training.</li> </ul> <p>If you encounter any issues or have questions, please visit our Support Page or contact us at support@analitiq-app.com.</p> <p>We are excited to have you on board and look forward to helping you make the most of your data with Analitiq!</p> <p>Thank you for choosing Analitiq!</p> <p>The Analitiq Team</p>"},{"location":"getting_started/hosted/profiles/","title":"Profiles Configuration (<code>profiles.yml</code>) for Analitiq","text":"<p>The file <code>profiles.yml</code> has all of your sensitive connection information, such as your API keys and DB credentials, so treat it with respect. This file can be placed in one of the two locations: 1. in the <code>.analitiq</code> directory inside your users home directory 2. in the root directory of your project</p> <p>We recommend the 1st option so it is out of the way and not accidentally picked up by <code>git</code>.</p> <p>NOTE:  If you choose to keep profiles.yml file in your project directory, make sure to add it to your gitignore.</p>"},{"location":"getting_started/hosted/profiles/#sample-file","title":"Sample File","text":"<p>Here is how the <code>profiles.yml</code> should look like: <pre><code>test:\n  connections:\n    databases:\n      - name: prod_dw\n        type: postgres\n        host: xxxxx\n        user: xxxx\n        password: 'xxxxx'\n        port: 5432\n        dbname: sample_db\n        dbschema: sample_schema\n        threads: 4\n        keepalives_idle: 240 # default 240 seconds\n        connect_timeout: 10 # default 10 seconds\n        # search_path: public # optional, not recommended\n    llms:\n      - name: prod_llm\n        type: openai\n        api_key: xxxxxx\n        temperature: 0.0\n        llm_model_name: gpt-3.5-turbo\n      - name: dev_llm\n        type: mistral\n        api_key: xxxxxx\n      - name: aws_llm\n        type: bedrock\n        credentials_profile_name: my_profile\n        provider: anthropic\n        llm_model_name: anthropic.claude-v2:1\n        temperature: 0.0\n    vector_dbs:\n      - name: prod_vdb\n        type: weaviate\n        host: example.com\n        api_key: xxxxx\n\n  usage:\n    databases: prod_dw\n    llms: aws_llm\n    vector_dbs: prod_vdb\n</code></pre></p>"},{"location":"getting_started/hosted/profiles/#sections","title":"Sections","text":"<p>The sections present in the <code>profiles.yaml</code> are divided into 3 groups: 1. Database connections 2. LLM API connections 3. Vector Database connections</p> <p>We first start with the name of the profile as a root identifier. In our example it is creatively named <code>test</code>. Each set of configurations begins under profile identifier. At the next level, we put <code>connections</code>: This section is designed to establish and configure various types of connections.</p>"},{"location":"getting_started/hosted/profiles/#databases","title":"Databases","text":"<p>The <code>connections.databases</code> subsection configures database connections. Each connection is defined as a list item. - <code>name</code>: Identifier for the connection. - <code>type</code>: Defines the type of the database. In the provided example it's a postgres database. - <code>host</code>: The hostname or IP address where the database is located. - <code>user</code>: The username used to access the database. - <code>password</code>: The password associated with the username. - <code>port</code>: The port number to connect to the database. - <code>db_name</code>: The name of the database to connect to. - <code>db_schema</code>: A list containing the database schemas that can be used by Analitiq. - <code>threads</code>: Number of threads to use. - <code>keepalives_idle</code>: The idle interval before the connection is dropped. The default value is 240 seconds. - <code>connect_timeout</code>: Connection timeout value in seconds. The default is 10 seconds.</p>"},{"location":"getting_started/hosted/profiles/#llms","title":"LLMs","text":"<p>The <code>connections.llms</code> section is used to configure different Large Language Model connections.</p> <ul> <li><code>name</code>: The name of the llm connection.</li> <li><code>type</code>: The type of service providing the llm, such as openai or mistral.</li> <li><code>api_key</code>: The key used to authenticate with the service.</li> <li><code>llm_model_name</code>: The specific model to be used, provided by the llm service.</li> </ul>"},{"location":"getting_started/hosted/profiles/#vector-databases","title":"Vector Databases","text":"<p>The <code>connections.vector_dbs</code> section configures connections to vector databases.</p> <ul> <li><code>name</code>: The name to identify the vector database connection.</li> <li><code>type</code>: The type of service that provides vector databases.</li> <li><code>host</code>: The host address of the vector database service.</li> <li><code>api_key</code>: The API key for the vector databases service.</li> <li><code>usage</code>: This section defines the default connections out of the many that were configured in the previous sections.</li> <li><code>databases</code>: The name of the database connection to use by default.</li> <li><code>llms</code>: The name of the llm connection to be used by default.</li> <li><code>vector_dbs</code>: The name of the default vector database connection.</li> </ul>"},{"location":"getting_started/hosted/profiles/#usage","title":"Usage","text":"<p>The usage section specifies which db connection, LLM connection and vector database connection to use in a particular environment. This way, you can have dedicated <code>profiles.yml</code> files in each of the local/dev/integr/prod environments and define which connection each environment uses in the <code>usage</code> section. <pre><code>  usage:\n    databases: prod_dw\n    llms: aws_llm\n    vector_dbs: prod_vdb\n</code></pre></p>"},{"location":"getting_started/hosted/profiles/#example","title":"Example","text":"<p>Let's look at some examples. Let's say when I run Analitiq locally, I want to use OpenAI. And when I upload it to production server, I want to use Bedrock. I will set up my connections in <code>profiles.yml</code> in the following way: <pre><code>prod:\n  connections:\n    databases:\n      - name: prod_db\n        type: postgres\n        host: xxxx\n        user: xxxx\n        password: xxxx\n        port: 5432\n        dbname: postgres\n        dbschema: sample_data\n        threads: 4\n        keepalives_idle: 240 # default 240 seconds\n        connect_timeout: 10 # default 10 seconds\n        # search_path: public # optional, not recommended\n\n    llms:\n      - name: aws_llm\n        type: bedrock\n        credentials_profile_name: bedrock\n        region_name: eu-central-1\n        provider: anthropic\n        llm_model_name: anthropic.claude-v2\n        temperature: 0.0\n        aws_access_key_id: xxxxx\n        aws_secret_access_key: xxxxx\n  usage:\n    databases: prod_db\n    llms: aws_llm\n\nlocal:\n  connections:\n    databases:\n      - name: local_db\n        type: postgres\n        host: xxxx\n        user: xxxx\n        password: xxxx\n        port: 5432\n        dbname: postgres\n        dbschema: sample_data\n        threads: 4\n        keepalives_idle: 240 # default 240 seconds\n        connect_timeout: 10 # default 10 seconds\n        # search_path: public # optional, not recommended\n\n    llms:\n      - name: openai_llm\n        type: openai\n        api_key: xxxx\n        temperature: 0.0\n        llm_model_name: gpt-3.5-turbo\n  usage:\n    databases: local_db\n    llms: openai_llm\n</code></pre></p> <p>on my local machine, I would have <code>project.py</code> file with the following configuration <pre><code>name: 'analitiq'\nversion: '0.1'\nprofile: 'local'\nconfig_version: 2\n</code></pre> and the production server will have <code>project.py</code> file with the following configuration <pre><code>name: 'analitiq'\nversion: '0.1'\nprofile: 'prod'\nconfig_version: 2\n</code></pre></p> <p>Now, I can move the project files between my prod environment and local and Analitiq will use different run configurations in each environment.</p>"},{"location":"getting_started/hosted/project/","title":"Project Configuration (<code>project.yml</code>) for Analitiq","text":""},{"location":"getting_started/hosted/project/#overview","title":"Overview","text":"<p>The <code>project.yml</code> file serves as the main configuration file for an Analitiq project. It must be located in the project's root directory. This file specifies various parameters that define the behavior and settings of your Analitiq application. The file <code>project.yml</code> has all of your project data, such as where the logs are stored.  Most importantly, <code>project.yml</code> defines where your custom Services are located so Analitiq can pick them up and use them to manage your data.</p> <p></p> <pre><code>name: 'analitiq'\nversion: '0.1'\nprofile: 'test'\nconfig_version: 2\n\nconfig:\n  general:\n    chat_log_dir: \"chats\" # this is where we save our chat logs.\n    sql_dir: \"analysis\" # this is where the ETL SQLs are being saved and managed\n    services_dir: \"custom_agents\"\n    session_uuid_file: 'session_uuid.txt' # Where session identifier is being recorded. When session is reset, it is like beginning of a new chat topic and new log file will be created.\n    target_path: \"target\"\n    message_lookback: 5 # when LLM has no clue about users request, or users request relates to some item in chat history, how far back (in number of messages) should the LLM look in the current session chat log\n  vectordb:\n    doc_chunk_size: 2000\n    doc_chunk_overlap: 200\n\nservices:\n  - name: ChartService\n    description: \"Use this service to generate script for APEX charts to visualize data\"\n    path: \"custom_agents/chart/chart.py\"\n    class: \"Chart\"\n    method: \"run\"\n    inputs: \"dataframe as serialized json\"\n    outputs: \"javascript that is used by the frontend to visualize data\"\n</code></pre>"},{"location":"getting_started/hosted/project/#key-sections-and-parameters","title":"Key Sections and Parameters","text":"<ul> <li><code>name</code>: The name of your project.</li> <li><code>version</code>: The version of your project.</li> <li><code>profile</code>: Defines which profile from the profiles.yml is being used by this project.</li> <li><code>config_version</code>: (Not used currently) Placeholder for future use.</li> </ul>"},{"location":"getting_started/hosted/project/#general-configuration","title":"General Configuration","text":"<ul> <li><code>chat_log_dir</code>: Directory for saving chat logs.</li> <li><code>sql_dir</code>: Storage for ETL SQL scripts.</li> <li><code>services_dir</code>: Directory for custom services.</li> <li><code>session_uuid_file</code>: Tracks session identifiers.</li> <li><code>message_lookback</code>: Defines how far back the AI should refer in the conversation history when needed.</li> </ul>"},{"location":"getting_started/hosted/project/#vector-database-configuration","title":"Vector Database Configuration","text":"<ul> <li><code>doc_chunk_size</code>: Size of document chunks in characters.</li> <li><code>doc_chunk_overlap</code>: Overlap in characters between document chunks for consistency.</li> </ul>"},{"location":"getting_started/hosted/project/#services-configuration","title":"Services Configuration","text":"<p>Services define tasks like data visualization, PDF parsing, or API interactions. Each service is detailed with its functionality, input, and output specifications.</p>"},{"location":"getting_started/hosted/project/#example-chartservice","title":"Example: ChartService","text":"<ul> <li><code>path</code>: Location of the service script.</li> <li><code>class &amp; method</code>: Specifies the class and method to execute the service.</li> <li><code>inputs &amp; outputs</code>: Defines the expected inputs and the format of outputs.</li> </ul>"},{"location":"getting_started/hosted/project/#usage","title":"Usage","text":"<p>This configuration file enables the Analitiq framework to understand and manage project-specific settings and services efficiently. It is vital for running custom services and managing data interactions within the framework.</p>"},{"location":"getting_started/hosted/quick_start/","title":"Quick Start","text":"<ol> <li>Clone the Git repo Analitiq AI</li> <li>Set up <code>profiles.yml</code>. In order to understand where and how to place it, please read Profiles Configuration</li> <li>Set up <code>project.yml</code> in root directory. In order to understand more about it, please check out Project Configuration</li> <li>Edit and run the example file <code>libs/cookbooks/example_analitiq.py</code>, to ask your first question.</li> </ol>"},{"location":"getting_started/hosted/quick_start/#configuration-files","title":"Configuration files","text":"<p>There are 2 configuration files:</p> <ol> <li> <p><code>profiles.yaml</code> - this file has all the secrets and connections needed to connect to LLMs, VectorDBs, Databases. Because you may have different production and development environments, profiles.yaml allows you to define multiple profiles (and multiple credentials).</p> </li> <li> <p><code>project.yaml</code> - this file has the parameters needed for your particular project, including what profile to use. You can define the profile in <code>profile</code> parameter.    Once you have your project deployed, you can specify which profile to be used by that particular project in <code>project.yaml</code>.</p> </li> </ol>"}]}